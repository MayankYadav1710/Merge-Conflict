{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7760820,"sourceType":"datasetVersion","datasetId":4527644},{"sourceId":10180289,"sourceType":"datasetVersion","datasetId":6246633,"isSourceIdPinned":false},{"sourceId":14690105,"sourceType":"datasetVersion","datasetId":9384478},{"sourceId":14690164,"sourceType":"datasetVersion","datasetId":9384523},{"sourceId":14690195,"sourceType":"datasetVersion","datasetId":9384533},{"sourceId":14690212,"sourceType":"datasetVersion","datasetId":9384546}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"pkdarabi/helmet\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:31:09.808900Z","iopub.execute_input":"2026-01-31T21:31:09.809114Z","iopub.status.idle":"2026-01-31T21:31:10.637515Z","shell.execute_reply.started":"2026-01-31T21:31:09.809092Z","shell.execute_reply":"2026-01-31T21:31:10.636683Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install the YOLOv8/YOLO11 library\n!pip install ultralytics\nimport os\nfrom ultralytics import YOLO\nfrom IPython.display import Image, display","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:31:10.638454Z","iopub.execute_input":"2026-01-31T21:31:10.638843Z","iopub.status.idle":"2026-01-31T21:31:21.112012Z","shell.execute_reply.started":"2026-01-31T21:31:10.638807Z","shell.execute_reply":"2026-01-31T21:31:21.111384Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import yaml\n\n# CONFIGURATION: Change these to match your specific folder names in /kaggle/input\n# Tip: Look at the \"Data\" sidebar on the right in Kaggle to copy the exact path\ndataset_root = '//kaggle/input/helmet' \n\n# Create a dictionary for the data.yaml content\ndata_yaml_content = {\n    'train': os.path.join(dataset_root, 'train/images'),\n    'val': os.path.join(dataset_root, 'valid/images'),\n    'test': os.path.join(dataset_root, 'test/images'),\n    \n    # These must match the number of classes in your dataset\n    'nc': 2, \n    'names': ['Helmet', 'No-Helmet'] \n}\n\n# Save this file to the working directory (which is writable)\nwith open('/kaggle/working/data.yaml', 'w') as f:\n    yaml.dump(data_yaml_content, f)\n\nprint(\"data.yaml created successfully at /kaggle/working/data.yaml\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:31:21.112961Z","iopub.execute_input":"2026-01-31T21:31:21.113380Z","iopub.status.idle":"2026-01-31T21:31:21.119584Z","shell.execute_reply.started":"2026-01-31T21:31:21.113334Z","shell.execute_reply":"2026-01-31T21:31:21.118848Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import yaml\nimport os\n\n# --- ‚ö†Ô∏è CHANGE THIS LINE ‚ö†Ô∏è ---\n# Paste the actual folder name you found in Step 2.\n# Example: dataset_dir_name = 'helmet-detection-dataset'\ndataset_dir_name = '/kaggle/input/helmet' \n# ------------------------------\n\nbase_path = f'/kaggle/input/{dataset_dir_name}'\n\n# Auto-detect if the validation folder is named 'val' or 'valid'\n# This prevents errors if your dataset uses different naming conventions\nval_folder = 'valid' if os.path.exists(os.path.join(base_path, 'valid')) else 'val'\n\ndata_yaml_content = {\n    'train': os.path.join(base_path, 'train/images'),\n    'val': os.path.join(base_path, f'{val_folder}/images'), # Uses the detected name\n    'test': os.path.join(base_path, 'test/images'),\n    'nc': 2,\n    'names': ['Helmet', 'No-Helmet']\n}\n\n# Write the new data.yaml\nwith open('/kaggle/working/data.yaml', 'w') as f:\n    yaml.dump(data_yaml_content, f)\n\nprint(f\"fixed data.yaml created! \\nTrain path: {data_yaml_content['train']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:31:21.120449Z","iopub.execute_input":"2026-01-31T21:31:21.120674Z","iopub.status.idle":"2026-01-31T21:31:21.139080Z","shell.execute_reply.started":"2026-01-31T21:31:21.120652Z","shell.execute_reply":"2026-01-31T21:31:21.138377Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport yaml\n\n# 1. Define your base path\nbase_path = '/kaggle/input/helmet'\n\n# 2. Automatically find if the validation folder is named \"val\" or \"valid\"\n# (This prevents the common \"missing path\" error)\nif os.path.exists(os.path.join(base_path, 'valid')):\n    val_dir = 'valid'\nelif os.path.exists(os.path.join(base_path, 'val')):\n    val_dir = 'val'\nelse:\n    # Fallback: Look inside subfolders if dataset is nested\n    print(\"Warning: Standard folders not found. Listing content:\")\n    print(os.listdir(base_path))\n    val_dir = 'valid' # Defaulting, might need manual adjustment if nested\n\nprint(f\"Detected validation folder name: {val_dir}\")\n\n# 3. Create the configuration dictionary\ndata_yaml_content = {\n    'train': os.path.join(base_path, 'train/images'),\n    'val': os.path.join(base_path, val_dir, 'images'),\n    'test': os.path.join(base_path, 'test/images'), # Optional\n    'nc': 2,\n    'names': ['Helmet', 'No-Helmet']\n}\n\n# 4. Save to the writable working directory\nwith open('/kaggle/working/data.yaml', 'w') as f:\n    yaml.dump(data_yaml_content, f)\n\nprint(\"\\nSUCCESS: /kaggle/working/data.yaml has been created!\")\nprint(f\"Train path set to: {data_yaml_content['train']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:31:21.139999Z","iopub.execute_input":"2026-01-31T21:31:21.140301Z","iopub.status.idle":"2026-01-31T21:31:21.161736Z","shell.execute_reply.started":"2026-01-31T21:31:21.140265Z","shell.execute_reply":"2026-01-31T21:31:21.161041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport yaml\n\n# 1. We know the root is here\nroot = '/kaggle/input/helmet'\n\n# 2. Check which subfolder actually contains the 'train' data\n# We check both 'HelmetViolations' and 'HelmetViolationsV2'\npossible_subfolders = ['HelmetViolations', 'HelmetViolationsV2']\ndataset_folder = None\n\nfor sub in possible_subfolders:\n    # We look for the 'train' folder inside\n    test_path = os.path.join(root, sub, 'train')\n    if os.path.exists(test_path):\n        dataset_folder = os.path.join(root, sub)\n        print(f\"‚úÖ Found dataset in: {dataset_folder}\")\n        break\n\nif not dataset_folder:\n    print(\"‚ùå Could not find a 'train' folder in either subfolder. Please check manually.\")\nelse:\n    # 3. Handle 'valid' vs 'val' naming\n    val_dir = 'valid' if os.path.exists(os.path.join(dataset_folder, 'valid')) else 'val'\n    \n    # 4. Create the CORRECT data.yaml\n    data_yaml_content = {\n        'train': os.path.join(dataset_folder, 'train/images'),\n        'val': os.path.join(dataset_folder, val_dir, 'images'),\n        'test': os.path.join(dataset_folder, 'test/images'),\n        'nc': 2,\n        'names': ['Helmet', 'No-Helmet']\n    }\n\n    with open('/kaggle/working/data.yaml', 'w') as f:\n        yaml.dump(data_yaml_content, f)\n\n    print(\"\\nSUCCESS! New data.yaml created.\")\n    print(f\"Training path is set to: {data_yaml_content['train']}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:31:21.163693Z","iopub.execute_input":"2026-01-31T21:31:21.164082Z","iopub.status.idle":"2026-01-31T21:31:21.173660Z","shell.execute_reply.started":"2026-01-31T21:31:21.164057Z","shell.execute_reply":"2026-01-31T21:31:21.173080Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\n\nmodel = YOLO('yolo11n.pt') \n\nresults = model.train(\n    data='/kaggle/working/data.yaml',\n    epochs=10,\n    imgsz=640,\n    batch=16,\n    project='helmet_project',\n    name='try_1'\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:31:21.174494Z","iopub.execute_input":"2026-01-31T21:31:21.174777Z","iopub.status.idle":"2026-01-31T21:33:42.080351Z","shell.execute_reply.started":"2026-01-31T21:31:21.174743Z","shell.execute_reply":"2026-01-31T21:33:42.079625Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nprint(\"--- SCANNING FOR TRAINING RESULTS ---\")\nfound_results = False\nfound_weights = False\n\n# Walk through the entire working directory\nfor root, dirs, files in os.walk('/kaggle/working'):\n    for file in files:\n        # Check for the results graph\n        if file == 'results.png':\n            print(f\"‚úÖ Found Results Graph: {os.path.join(root, file)}\")\n            found_results = True\n        \n        # Check for the model weights (proof training worked)\n        if file == 'best.pt':\n            print(f\"‚úÖ Found Model Weights: {os.path.join(root, file)}\")\n            found_weights = True\n\nif not found_results and not found_weights:\n    print(\"‚ùå No training results found. The training likely crashed or didn't start.\")\n    print(\"\\n--- Listing All Folders (to help debug) ---\")\n    for root, dirs, files in os.walk('/kaggle/working'):\n        print(f\"Folder: {root}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:33:42.081543Z","iopub.execute_input":"2026-01-31T21:33:42.082019Z","iopub.status.idle":"2026-01-31T21:33:42.089087Z","shell.execute_reply.started":"2026-01-31T21:33:42.081983Z","shell.execute_reply":"2026-01-31T21:33:42.088372Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Image, display\n\n# The CORRECT path found by your scan\nrun_folder = '/kaggle/working/runs/detect/helmet_project/try_1'\n\nprint(\"--- Training Metrics ---\")\n# This graph shows if the model is getting smarter (loss going down)\ndisplay(Image(filename=f'{run_folder}/results.png'))\n\nprint(\"\\n--- Validation Batch (What the model 'sees') ---\")\n# This shows sample detections on data it used to test itself\n# Note: YOLO sometimes names this 'val_batch0_labels.jpg' or 'val_batch0_pred.jpg'\ntry:\n    display(Image(filename=f'{run_folder}/val_batch0_pred.jpg'))\nexcept FileNotFoundError:\n    print(\"Prediction image not found (filename might vary slightly).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:33:42.090094Z","iopub.execute_input":"2026-01-31T21:33:42.090507Z","iopub.status.idle":"2026-01-31T21:33:42.214181Z","shell.execute_reply.started":"2026-01-31T21:33:42.090483Z","shell.execute_reply":"2026-01-31T21:33:42.213443Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\nimport cv2\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n\n# 1. Load the model using the FOUND weights path\nmodel_path = '/kaggle/working/runs/detect/helmet_project/try_1/weights/best.pt'\nmodel = YOLO(model_path)\n\n# 2. Define path to test images (using the path we confirmed earlier)\n# We will use the 'test' folder from your dataset\ntest_images_path = '/kaggle/input/helmet/HelmetViolations/test/images'\nif not os.path.exists(test_images_path):\n    test_images_path = '/kaggle/input/helmet/HelmetViolationsV2/test/images'\n\n# 3. Initialize Data Log\nviolation_log = []\n\n# 4. Get list of images\nimage_files = [f for f in os.listdir(test_images_path) if f.endswith(('.jpg', '.png'))]\n# Let's process just 5 images to start\nimage_files = image_files[:5]\n\nprint(f\"üöÄ Model loaded! Processing {len(image_files)} images...\\n\")\n\nfor img_file in image_files:\n    full_path = os.path.join(test_images_path, img_file)\n    \n    # Run YOLO inference\n    results = model.predict(full_path, conf=0.4, verbose=False) \n    result = results[0]\n    \n    violation_detected = False\n    \n    # Check detections\n    for box in result.boxes:\n        cls_id = int(box.cls[0])\n        class_name = result.names[cls_id]\n        \n        # LOGIC: If 'No-Helmet' is found -> It's a violation\n        # (Double check: In your data.yaml, is 'No-Helmet' definitely one of the classes?)\n        if class_name == 'No-Helmet':\n            violation_detected = True\n            \n            violation_log.append({\n                'Image': img_file,\n                'Violation': 'No Helmet',\n                'Confidence': f\"{float(box.conf[0]):.2f}\",\n                'Action': 'Needs Number Plate OCR' \n            })\n\n    # Show result ONLY if violation found\n    if violation_detected:\n        print(f\"üö® VIOLATION DETECTED in: {img_file}\")\n        \n        # Draw the boxes using YOLO's built-in plotter\n        plotted_img = result.plot()\n        \n        # Display using Matplotlib\n        plt.figure(figsize=(8, 8))\n        plt.imshow(cv2.cvtColor(plotted_img, cv2.COLOR_BGR2RGB))\n        plt.axis('off')\n        plt.show()\n\n# 5. Create Excel Report\nif violation_log:\n    df = pd.DataFrame(violation_log)\n    df.to_excel('violation_report.xlsx', index=False)\n    print(\"\\n‚úÖ Excel Report Generated: 'violation_report.xlsx'\")\n    print(df)\nelse:\n    print(\"\\n‚úÖ No violations found in these 5 images (or class names didn't match).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:33:42.215249Z","iopub.execute_input":"2026-01-31T21:33:42.215753Z","iopub.status.idle":"2026-01-31T21:33:42.495108Z","shell.execute_reply.started":"2026-01-31T21:33:42.215728Z","shell.execute_reply":"2026-01-31T21:33:42.494305Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\nimport os\n\n# 1. Load Model\nmodel_path = '/kaggle/working/runs/detect/helmet_project/try_1/weights/best.pt'\nmodel = YOLO(model_path)\n\n# 2. PRINT THE CLASS NAMES (This is the key!)\nprint(f\"üìã Model Class Names: {model.names}\")\n\n# 3. Test on images again\ntest_images_path = '/kaggle/input/helmet/HelmetViolations/test/images'\nif not os.path.exists(test_images_path):\n    test_images_path = '/kaggle/input/helmet/HelmetViolationsV2/test/images'\n\nimage_files = [f for f in os.listdir(test_images_path) if f.endswith('.jpg')][:3]\n\nprint(f\"\\nScanning {len(image_files)} images (Confidence > 0.1)...\")\n\nfor img_file in image_files:\n    full_path = os.path.join(test_images_path, img_file)\n    \n    # Run with very low confidence just to see EVERYTHING\n    results = model.predict(full_path, conf=0.1, verbose=False)\n    \n    # Print what was found\n    if len(results[0].boxes) > 0:\n        print(f\"\\nüì∏ Image: {img_file}\")\n        for box in results[0].boxes:\n            cls_id = int(box.cls[0])\n            name = results[0].names[cls_id]\n            conf = float(box.conf[0])\n            print(f\"   -> Found: '{name}' (Confidence: {conf:.2f})\")\n    else:\n        print(f\"\\nüì∏ Image: {img_file} - Nothing detected.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:33:42.496186Z","iopub.execute_input":"2026-01-31T21:33:42.496714Z","iopub.status.idle":"2026-01-31T21:33:42.641084Z","shell.execute_reply.started":"2026-01-31T21:33:42.496687Z","shell.execute_reply":"2026-01-31T21:33:42.640287Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install easyocr","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:33:42.642161Z","iopub.execute_input":"2026-01-31T21:33:42.642740Z","iopub.status.idle":"2026-01-31T21:33:45.968159Z","shell.execute_reply.started":"2026-01-31T21:33:42.642713Z","shell.execute_reply":"2026-01-31T21:33:45.967413Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\nimport cv2\nimport easyocr\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n\n# --- CONFIGURATION ---\nmodel_path = '/kaggle/working/runs/detect/helmet_project/try_1/weights/best.pt'\ntest_images_path = '/kaggle/input/helmet/HelmetViolations/test/images'\nif not os.path.exists(test_images_path):\n    test_images_path = '/kaggle/input/helmet/HelmetViolationsV2/test/images'\n\n# 1. Initialize Tools\nprint(\"‚è≥ Loading YOLO model and OCR reader... (This takes 10s)\")\nmodel = YOLO(model_path)\nreader = easyocr.Reader(['en']) # Load English OCR\nprint(\"‚úÖ System Ready!\")\n\n# 2. Setup Logging\nviolation_log = []\nmax_images_to_check = 50 # Check more images to ensure we find a violation!\n\n# 3. Get Images\nimage_files = [f for f in os.listdir(test_images_path) if f.endswith(('.jpg', '.png'))]\nprint(f\"üìÇ Found {len(image_files)} images. Scanning the first {max_images_to_check} for violations...\\n\")\n\n# 4. Processing Loop\nfor i, img_file in enumerate(image_files[:max_images_to_check]):\n    full_path = os.path.join(test_images_path, img_file)\n    \n    # Run Detection\n    results = model.predict(full_path, conf=0.5, verbose=False)\n    result = results[0]\n    \n    violation_found = False\n    \n    # Check for \"No-Helmet\" class\n    for box in result.boxes:\n        cls_id = int(box.cls[0])\n        class_name = result.names[cls_id]\n        \n        if class_name == 'No-Helmet':\n            violation_found = True\n            \n            # --- VIOLATION LOGIC ---\n            print(f\"üö® VIOLATION DETECTED in {img_file}\")\n            \n            # A. Load Image for OCR\n            img = cv2.imread(full_path)\n            height, width, _ = img.shape\n            \n            # B. OCR Strategy: Scan the bottom half of the image (where plates usually are)\n            # Cropping: [y:y+h, x:x+w]\n            plate_region = img[int(height/2):height, 0:width] \n            \n            # C. Read Text\n            ocr_result = reader.readtext(plate_region)\n            \n            # Extract text (if any found)\n            detected_text = \"Unknown\"\n            if len(ocr_result) > 0:\n                # Get the text with highest confidence\n                detected_text = ocr_result[0][1]\n                print(f\"   üîç Potential Number Plate: {detected_text}\")\n            else:\n                print(\"   ‚ö†Ô∏è No clear text found (Image might be blurry)\")\n\n            # D. Log to Database\n            violation_log.append({\n                'Filename': img_file,\n                'Violation Type': 'No Helmet',\n                'Confidence': f\"{float(box.conf[0]):.2f}\",\n                'Detected NumberPlate': detected_text\n            })\n\n            # E. Show Proof\n            plt.figure(figsize=(5, 5))\n            plt.imshow(cv2.cvtColor(result.plot(), cv2.COLOR_BGR2RGB))\n            plt.title(f\"Violation: {detected_text}\")\n            plt.axis('off')\n            plt.show()\n            \n            # Stop checking this image (we found the violation)\n            break \n\n# 5. Save Final Report\nif len(violation_log) > 0:\n    df = pd.DataFrame(violation_log)\n    df.to_excel('Traffic_Violations.xlsx', index=False)\n    print(\"\\n‚úÖ SUCCESS: 'Traffic_Violations.xlsx' has been created.\")\n    display(df)\nelse:\n    print(\"\\n‚úÖ Scan complete. No violations found in this batch. (Try increasing max_images_to_check)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:33:45.969734Z","iopub.execute_input":"2026-01-31T21:33:45.970201Z","iopub.status.idle":"2026-01-31T21:33:49.651062Z","shell.execute_reply.started":"2026-01-31T21:33:45.970152Z","shell.execute_reply":"2026-01-31T21:33:49.650383Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import kagglehub\n\n# Download latest version\npath = kagglehub.dataset_download(\"aneesarom/rider-with-helmet-without-helmet-number-plate\")\n\nprint(\"Path to dataset files:\", path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:33:49.652208Z","iopub.execute_input":"2026-01-31T21:33:49.652960Z","iopub.status.idle":"2026-01-31T21:33:50.047004Z","shell.execute_reply.started":"2026-01-31T21:33:49.652922Z","shell.execute_reply":"2026-01-31T21:33:50.046374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport yaml\n\n# 1. Your NEW Dataset Path\nroot = '/kaggle/input/rider-with-helmet-without-helmet-number-plate'\n\n# 2. Inspect folder structure to find 'train'\n# We search recursively because sometimes datasets are nested like \"dataset/dataset/train\"\ntrain_dir = None\nval_dir = None\n\nfor dirpath, dirnames, filenames in os.walk(root):\n    if 'train' in dirnames and 'images' in os.listdir(os.path.join(dirpath, 'train')):\n        train_dir = os.path.join(dirpath, 'train', 'images')\n        base_dir = dirpath\n        \n        # Look for validation folder in the same place\n        if 'valid' in dirnames:\n            val_dir = os.path.join(dirpath, 'valid', 'images')\n        elif 'val' in dirnames:\n            val_dir = os.path.join(dirpath, 'val', 'images')\n        break\n\nif train_dir:\n    print(f\"‚úÖ Found Dataset at: {base_dir}\")\n    \n    # 3. Create the data.yaml\n    data_yaml_content = {\n        'train': train_dir,\n        'val': val_dir if val_dir else train_dir, # Fallback to train if val missing\n        'nc': 2, # Assuming 2 classes (Helmet/No-Helmet) - adjust if dataset has more\n        'names': ['Helmet', 'No-Helmet']\n    }\n\n    with open('/kaggle/working/data_new.yaml', 'w') as f:\n        yaml.dump(data_yaml_content, f)\n\n    print(\"‚úÖ 'data_new.yaml' created successfully!\")\nelse:\n    print(\"‚ùå Could not find a 'train/images' folder. Please check the dataset structure manually.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:33:50.048042Z","iopub.execute_input":"2026-01-31T21:33:50.048547Z","iopub.status.idle":"2026-01-31T21:33:50.064078Z","shell.execute_reply.started":"2026-01-31T21:33:50.048506Z","shell.execute_reply":"2026-01-31T21:33:50.063470Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport glob\nimport yaml\n\n# 1. Define where the labels are (based on your error log)\nlabel_path = '/kaggle/input/rider-with-helmet-without-helmet-number-plate/train/labels'\nimage_path = '/kaggle/input/rider-with-helmet-without-helmet-number-plate/train/images'\n\n# 2. Scan for unique Class IDs\nprint(f\"üîç Scanning labels in {label_path}...\")\nlabel_files = glob.glob(os.path.join(label_path, '*.txt'))\nunique_classes = set()\n\nfor f in label_files:\n    with open(f, 'r') as file:\n        lines = file.readlines()\n        for line in lines:\n            parts = line.strip().split()\n            if parts:\n                class_id = int(parts[0])\n                unique_classes.add(class_id)\n\nif not unique_classes:\n    print(\"‚ùå No labels found! Check the path.\")\nelse:\n    max_class_id = max(unique_classes)\n    required_nc = max_class_id + 1\n    \n    print(f\"‚úÖ Found Class IDs: {sorted(list(unique_classes))}\")\n    print(f\"‚úÖ Max Class ID is {max_class_id}. We need nc={required_nc}\")\n\n    # 3. Create the CORRECT data_new.yaml\n    # We use generic names for now, we will map them later\n    class_names = [f\"Class_{i}\" for i in range(required_nc)]\n    \n    data_yaml_content = {\n        'train': image_path,\n        # If val folder doesn't exist, use train for both just to get it running\n        'val': image_path.replace('train', 'valid') if os.path.exists(image_path.replace('train', 'valid')) else image_path,\n        'nc': required_nc,\n        'names': class_names\n    }\n\n    with open('/kaggle/working/data_new.yaml', 'w') as f:\n        yaml.dump(data_yaml_content, f)\n\n    print(\"\\nüéâ Fixed 'data_new.yaml' created successfully!\")\n    print(f\"You can now train with nc={required_nc}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:33:50.065008Z","iopub.execute_input":"2026-01-31T21:33:50.065242Z","iopub.status.idle":"2026-01-31T21:33:50.603270Z","shell.execute_reply.started":"2026-01-31T21:33:50.065204Z","shell.execute_reply":"2026-01-31T21:33:50.602487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\n\n# Load model\nmodel = YOLO('yolo11n.pt') \n\n# Train\nresults = model.train(\n    data='/kaggle/working/data_new.yaml',\n    epochs=15,\n    imgsz=640,\n    batch=16,\n    project='helmet_project',\n    name='real_run_fixed' # Changed name to keep it separate\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:33:50.604449Z","iopub.execute_input":"2026-01-31T21:33:50.604771Z","iopub.status.idle":"2026-01-31T21:34:51.403457Z","shell.execute_reply.started":"2026-01-31T21:33:50.604735Z","shell.execute_reply":"2026-01-31T21:34:51.402525Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nprint(\"üîç Scanning for model weights...\")\nfound = False\nfor root, dirs, files in os.walk('/kaggle/working'):\n    for file in files:\n        if file == 'best.pt':\n            print(f\"‚úÖ FOUND IT: {os.path.join(root, file)}\")\n            found = True\n\nif not found:\n    print(\"‚ùå No 'best.pt' found. You must run the training block below.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:34:51.404928Z","iopub.execute_input":"2026-01-31T21:34:51.405286Z","iopub.status.idle":"2026-01-31T21:34:51.411653Z","shell.execute_reply.started":"2026-01-31T21:34:51.405236Z","shell.execute_reply":"2026-01-31T21:34:51.411033Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\nimport cv2\nimport matplotlib.pyplot as plt\nimport os\nimport random\nimport glob\n\n# 1. Use the EXACT path you just found\nmodel_path = '/kaggle/working/runs/detect/helmet_project/real_run_fixed/weights/best.pt'\nmodel = YOLO(model_path)\n\n# 2. Find images to test\n# We look for images recursively in your new dataset\nimage_files = glob.glob('/kaggle/input/rider-with-helmet-without-helmet-number-plate/**/*.jpg', recursive=True)\n\nif not image_files:\n    print(\"‚ùå No images found. Check the dataset path.\")\nelse:\n    # Pick 3 random images\n    test_samples = random.sample(image_files, 3)\n\n    print(f\"üïµÔ∏è Mapping Classes... Testing on 3 images.\\n\")\n\n    for img_path in test_samples:\n        # Run inference\n        results = model.predict(img_path, conf=0.3) # Low confidence to see everything\n        result = results[0]\n        \n        # Plot\n        plotted_img = result.plot()\n        \n        # Show Image\n        plt.figure(figsize=(8, 8))\n        plt.imshow(cv2.cvtColor(plotted_img, cv2.COLOR_BGR2RGB))\n        plt.axis('off')\n        plt.title(f\"Identify the Class IDs!\")\n        plt.show()\n        \n        # Print detected classes\n        found_classes = set()\n        for box in result.boxes:\n            cls_id = int(box.cls[0])\n            found_classes.add(f\"Class_{cls_id}\")\n        print(f\"üëâ Found in this image: {found_classes}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:34:51.412659Z","iopub.execute_input":"2026-01-31T21:34:51.412981Z","iopub.status.idle":"2026-01-31T21:34:52.429624Z","shell.execute_reply.started":"2026-01-31T21:34:51.412957Z","shell.execute_reply":"2026-01-31T21:34:52.428789Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\nimport cv2\nimport easyocr\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n\n# --- ‚öôÔ∏è CONFIGURATION (UPDATE THESE NUMBERS!) ‚öôÔ∏è ---\n# Look at your previous images to confirm these numbers\nVIOLATION_ID = 2   # Replace with the Class ID for 'No Helmet'\nPLATE_ID = 3       # Replace with the Class ID for 'Number Plate'\n\n# Path to your trained model\nmodel_path = '/kaggle/working/runs/detect/helmet_project/real_run_fixed/weights/best.pt'\n\n# Path to test images\ntest_images_path = '/kaggle/input/rider-with-helmet-without-helmet-number-plate/train/images' # Using train for demo as it has data\n# ---------------------------------------------------\n\n# 1. Initialize Tools\nprint(\"‚è≥ Loading System...\")\nmodel = YOLO(model_path)\nreader = easyocr.Reader(['en']) \nprint(\"‚úÖ System Ready!\")\n\n# 2. Setup Logging\nviolation_log = []\nimages_to_scan = [f for f in os.listdir(test_images_path) if f.endswith('.jpg')][:50] # Scan first 50\n\nprint(f\"üöÄ Scanning {len(images_to_scan)} images for Class {VIOLATION_ID} (Violations)...\")\n\nfor img_file in images_to_scan:\n    full_path = os.path.join(test_images_path, img_file)\n    \n    # Run Inference\n    results = model.predict(full_path, conf=0.4, verbose=False)\n    result = results[0]\n    \n    # Check if this image has a violation\n    has_violation = False\n    detected_plate_text = \"Not Found\"\n    \n    # First, look for the violation class\n    for box in result.boxes:\n        cls_id = int(box.cls[0])\n        if cls_id == VIOLATION_ID:\n            has_violation = True\n            break\n    \n    if has_violation:\n        print(f\"üö® VIOLATION FOUND in {img_file}\")\n        \n        # Now look for the Plate (Class 3) in the SAME image\n        img = cv2.imread(full_path)\n        \n        plate_found = False\n        for box in result.boxes:\n            cls_id = int(box.cls[0])\n            \n            # If we find a Number Plate box\n            if cls_id == PLATE_ID:\n                plate_found = True\n                x1, y1, x2, y2 = map(int, box.xyxy[0])\n                \n                # Crop strictly the plate area\n                plate_crop = img[y1:y2, x1:x2]\n                \n                # Clean the crop (optional: convert to gray for better OCR)\n                gray_plate = cv2.cvtColor(plate_crop, cv2.COLOR_BGR2GRAY)\n                \n                # Run OCR on the crop\n                ocr_out = reader.readtext(gray_plate)\n                \n                if ocr_out:\n                    # Taking the result with highest confidence\n                    detected_plate_text = ocr_out[0][1] \n                    print(f\"   üîç Plate Read: {detected_plate_text}\")\n                else:\n                    detected_plate_text = \"Unreadable\"\n                    print(\"   ‚ö†Ô∏è Plate detected but text unclear\")\n                \n                # Break after finding one plate (simplify for now)\n                break\n        \n        # Fallback: If no Plate Class detected, scan bottom area\n        if not plate_found:\n            print(\"   ‚ö†Ô∏è No Plate Box found. Scanning bottom area...\")\n            h, w, _ = img.shape\n            bottom_crop = img[int(h/2):h, 0:w]\n            ocr_out = reader.readtext(bottom_crop)\n            if ocr_out:\n                detected_plate_text = ocr_out[0][1]\n        \n        # Log it\n        violation_log.append({\n            'Filename': img_file,\n            'Violation Type': 'No Helmet',\n            'Plate Number': detected_plate_text\n        })\n        \n        # Show Proof\n        plt.figure(figsize=(5, 5))\n        plt.imshow(cv2.cvtColor(result.plot(), cv2.COLOR_BGR2RGB))\n        plt.title(f\"Plate: {detected_plate_text}\")\n        plt.axis('off')\n        plt.show()\n\n# 3. Save Final Excel Report\nif len(violation_log) > 0:\n    df = pd.DataFrame(violation_log)\n    df.to_excel('Traffic_Challan_Report.xlsx', index=False)\n    print(\"\\n‚úÖ SUCCESS: 'Traffic_Challan_Report.xlsx' generated. Download it from Output!\")\n    display(df)\nelse:\n    print(\"\\n‚úÖ Scan complete. No violations found in this batch.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:34:52.430842Z","iopub.execute_input":"2026-01-31T21:34:52.431508Z","iopub.status.idle":"2026-01-31T21:35:06.600556Z","shell.execute_reply.started":"2026-01-31T21:34:52.431469Z","shell.execute_reply":"2026-01-31T21:35:06.599949Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from ultralytics import YOLO\nimport cv2\nimport easyocr\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# --- ‚öôÔ∏è CONFIGURATION ‚öôÔ∏è ---\nVIOLATION_ID = 2   # 'No Helmet' ID\nPLATE_ID = 3       # 'Number Plate' ID\nmodel_path = '/kaggle/working/runs/detect/helmet_project/real_run_fixed/weights/best.pt'\ntest_images_path = '/kaggle/input/rider-with-helmet-without-helmet-number-plate/train/images'\n\n# --- üõ†Ô∏è HELPER FUNCTION: CLEAN THE PLATE üõ†Ô∏è ---\ndef preprocess_plate(plate_img):\n    # 1. Resize: Double the size of the plate (makes text clearer)\n    plate_img = cv2.resize(plate_img, None, fx=3, fy=3, interpolation=cv2.INTER_CUBIC)\n    \n    # 2. Grayscale: Remove color\n    gray = cv2.cvtColor(plate_img, cv2.COLOR_BGR2GRAY)\n    \n    # 3. Denoise and Threshold (Make it Black & White)\n    # This acts like a \"scanner\" to separate text from background\n    gray = cv2.bilateralFilter(gray, 11, 17, 17) \n    _, binary = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n    \n    return binary\n\n# ---------------------------------------------------\n\nprint(\"‚è≥ Loading System with Enhanced OCR...\")\nmodel = YOLO(model_path)\n# We add 'allowlist' to force it to look ONLY for standard license plate chars\nreader = easyocr.Reader(['en']) \nprint(\"‚úÖ System Ready!\")\n\nviolation_log = []\nimages_to_scan = [f for f in os.listdir(test_images_path) if f.endswith('.jpg')][:50]\n\nprint(f\"üöÄ Scanning {len(images_to_scan)} images...\")\n\nfor img_file in images_to_scan:\n    full_path = os.path.join(test_images_path, img_file)\n    results = model.predict(full_path, conf=0.3, verbose=False) # Lowered conf slightly\n    result = results[0]\n    \n    has_violation = False\n    \n    # Check for Violation\n    for box in result.boxes:\n        if int(box.cls[0]) == VIOLATION_ID:\n            has_violation = True\n            break\n            \n    if has_violation:\n        img = cv2.imread(full_path)\n        plate_text = \"Unreadable\"\n        \n        # Look for Plate\n        for box in result.boxes:\n            if int(box.cls[0]) == PLATE_ID:\n                x1, y1, x2, y2 = map(int, box.xyxy[0])\n                \n                # Crop\n                plate_crop = img[y1:y2, x1:x2]\n                \n                # ‚ú® APPLY PREPROCESSING ‚ú®\n                clean_plate = preprocess_plate(plate_crop)\n                \n                # Run OCR with ALLOWLIST (Only A-Z and 0-9)\n                ocr_out = reader.readtext(clean_plate, allowlist='ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n                \n                if ocr_out:\n                    # Pick the result with highest confidence\n                    best_match = max(ocr_out, key=lambda x: x[2])\n                    plate_text = best_match[1]\n                    print(f\"üö® {img_file} | üîç Raw Plate: {plate_text} (Conf: {best_match[2]:.2f})\")\n                    \n                    # VISUAL DEBUG: Show what the OCR actually saw\n                    plt.figure(figsize=(10, 4))\n                    \n                    # Show Original Crop\n                    plt.subplot(1, 2, 1)\n                    plt.imshow(cv2.cvtColor(plate_crop, cv2.COLOR_BGR2RGB))\n                    plt.title(\"Original Crop\")\n                    plt.axis('off')\n                    \n                    # Show Processed Crop (The one OCR read)\n                    plt.subplot(1, 2, 2)\n                    plt.imshow(clean_plate, cmap='gray')\n                    plt.title(f\"Cleaned Input: {plate_text}\")\n                    plt.axis('off')\n                    \n                    plt.show()\n                break\n        \n        violation_log.append({\n            'Filename': img_file,\n            'Violation': 'No Helmet',\n            'Number Plate': plate_text\n        })\n\n# Save Report\nif violation_log:\n    pd.DataFrame(violation_log).to_excel('Enhanced_Report.xlsx', index=False)\n    print(\"\\n‚úÖ Enhanced Report Generated!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:35:06.603924Z","iopub.execute_input":"2026-01-31T21:35:06.604620Z","iopub.status.idle":"2026-01-31T21:35:12.508107Z","shell.execute_reply.started":"2026-01-31T21:35:06.604592Z","shell.execute_reply":"2026-01-31T21:35:12.507376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nfrom ultralytics import YOLO\nimport easyocr\n\n# --- CONFIGURATION ---\n# Path to your trained model\nmodel_path = '/kaggle/working/runs/detect/helmet_project/real_run_fixed/weights/best.pt'\n# Path to a video file (Upload one to Kaggle Input first!)\nvideo_path = '/kaggle/input/your-video-folder/traffic_sample.mp4' \n# Output name\noutput_path = 'Final_Demo_Output.mp4'\n\n# IDs (Use the ones we confirmed earlier)\nVIOLATION_ID = 2\nPLATE_ID = 3\n\n# --- SETUP ---\nmodel = YOLO(model_path)\nreader = easyocr.Reader(['en'])\ncap = cv2.VideoCapture(video_path)\n\n# Get video properties for saving\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = int(cap.get(cv2.CAP_PROP_FPS))\n\n# Create Video Writer\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n\nprint(\"üé• Processing Video... This might take a while!\")\n\nwhile cap.isOpened():\n    success, frame = cap.read()\n    if not success:\n        break\n\n    # Run Inference\n    results = model.predict(frame, conf=0.4, verbose=False)\n    result = results[0]\n    \n    # Plot the detections on the frame\n    # This draws the boxes automatically\n    annotated_frame = result.plot() \n    \n    # Check for Violations to add OCR text\n    for box in result.boxes:\n        cls_id = int(box.cls[0])\n        if cls_id == VIOLATION_ID:\n            # We found a violator! Let's look for their plate in this frame\n            for plate_box in result.boxes:\n                if int(plate_box.cls[0]) == PLATE_ID:\n                    x1, y1, x2, y2 = map(int, plate_box.xyxy[0])\n                    \n                    # Crop & Read\n                    plate_crop = frame[y1:y2, x1:x2]\n                    try:\n                        gray = cv2.cvtColor(plate_crop, cv2.COLOR_BGR2GRAY)\n                        ocr_out = reader.readtext(gray, allowlist='ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n                        if ocr_out:\n                            text = ocr_out[0][1]\n                            # Draw the Plate Text on the video\n                            cv2.putText(annotated_frame, f\"Plate: {text}\", (x1, y1-10), \n                                        cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n                    except:\n                        pass # Skip if crop fails\n    \n    # Save frame\n    out.write(annotated_frame)\n\ncap.release()\nout.release()\nprint(f\"‚úÖ Video processing complete! Download '{output_path}' from Output.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:35:12.509248Z","iopub.execute_input":"2026-01-31T21:35:12.509623Z","iopub.status.idle":"2026-01-31T21:35:14.923101Z","shell.execute_reply.started":"2026-01-31T21:35:12.509597Z","shell.execute_reply":"2026-01-31T21:35:14.922414Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Video\n\n# Embed the video so it plays in the browser\nVideo(\"Final_Demo_Output.mp4\", embed=True, width=800)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:35:14.924156Z","iopub.execute_input":"2026-01-31T21:35:14.924576Z","iopub.status.idle":"2026-01-31T21:35:14.929672Z","shell.execute_reply.started":"2026-01-31T21:35:14.924537Z","shell.execute_reply":"2026-01-31T21:35:14.928965Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# 1. Convert the video to H.264 format (Browser Friendly)\n# This uses the command line tool 'ffmpeg'\n!ffmpeg -y -i Final_Demo_Output.mp4 -vcodec libx264 Final_Demo_Browser.mp4\n\n# 2. Play the NEW converted video\nfrom IPython.display import Video\nVideo(\"Final_Demo_Browser.mp4\", embed=True, width=800)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:35:14.930691Z","iopub.execute_input":"2026-01-31T21:35:14.931009Z","iopub.status.idle":"2026-01-31T21:35:15.597277Z","shell.execute_reply.started":"2026-01-31T21:35:14.930972Z","shell.execute_reply":"2026-01-31T21:35:15.596354Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Image, display\n\n# Path to your training results\n# (Make sure this matches your final run folder)\nresults_path = '/kaggle/working/runs/detect/helmet_project/real_run_fixed/results.png'\nconfusion_matrix_path = '/kaggle/working/runs/detect/helmet_project/real_run_fixed/confusion_matrix.png'\n\nprint(\"--- üìä Accuracy Metrics (mAP) ---\")\ndisplay(Image(filename=results_path))\n\nprint(\"\\n--- üß© Confusion Matrix (Where it gets confused) ---\")\n# This shows how often it mistakes a Helmet for a No-Helmet\ntry:\n    display(Image(filename=confusion_matrix_path))\nexcept:\n    print(\"Confusion matrix not found (requires validation run).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:35:15.598842Z","iopub.execute_input":"2026-01-31T21:35:15.599219Z","iopub.status.idle":"2026-01-31T21:35:15.614875Z","shell.execute_reply.started":"2026-01-31T21:35:15.599187Z","shell.execute_reply":"2026-01-31T21:35:15.614171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nprint(\"üìÇ Current Directory Contents:\")\nprint(os.listdir('/kaggle/working/'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:35:15.615886Z","iopub.execute_input":"2026-01-31T21:35:15.616534Z","iopub.status.idle":"2026-01-31T21:35:15.620634Z","shell.execute_reply.started":"2026-01-31T21:35:15.616506Z","shell.execute_reply":"2026-01-31T21:35:15.619782Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nfrom ultralytics import YOLO\nimport easyocr\nimport os\n\n# --- PATHS ---\n# We use the video we just downloaded in Step 1\nvideo_path = '/kaggle/working/test_video.mp4' \noutput_path = '/kaggle/working/Final_Demo_Output.mp4'\nmodel_path = '/kaggle/working/runs/detect/helmet_project/real_run_fixed/weights/best.pt'\n\n# IDs (From your previous findings)\nVIOLATION_ID = 2\nPLATE_ID = 3\n\n# 1. Initialize\nif not os.path.exists(video_path):\n    print(\"‚ùå Error: Run Step 1 first to download the video!\")\nelse:\n    print(f\"üìÇ Processing video: {video_path}\")\n    model = YOLO(model_path)\n    reader = easyocr.Reader(['en'])\n    cap = cv2.VideoCapture(video_path)\n    \n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    \n    # Setup Output\n    out = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n    \n    frame_count = 0\n    while cap.isOpened():\n        success, frame = cap.read()\n        if not success:\n            break\n\n        # Inference\n        results = model.predict(frame, conf=0.4, verbose=False)\n        result = results[0]\n        annotated_frame = result.plot()\n        \n        # (Optional) Add 'Plate' text if found\n        for box in result.boxes:\n            if int(box.cls[0]) == PLATE_ID:\n                x1, y1, x2, y2 = map(int, box.xyxy[0])\n                cv2.putText(annotated_frame, \"PLATE DETECTED\", (x1, y1-10), \n                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n        \n        out.write(annotated_frame)\n        frame_count += 1\n        if frame_count % 10 == 0:\n            print(f\"Processing frame {frame_count}...\", end='\\r')\n\n    cap.release()\n    out.release()\n    print(f\"\\n‚úÖ Done! Saved {frame_count} frames.\")\n    \n    # Convert for Browser Viewing\n    print(\"üîÑ Converting to H.264 for browser...\")\n    os.system(f\"ffmpeg -y -i {output_path} -vcodec libx264 /kaggle/working/Final_Browser_Video.mp4\")\n    \n    # Display\n    from IPython.display import Video\n    display(Video(\"/kaggle/working/Final_Browser_Video.mp4\", embed=True, width=800))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:35:15.621764Z","iopub.execute_input":"2026-01-31T21:35:15.622050Z","iopub.status.idle":"2026-01-31T21:35:15.637182Z","shell.execute_reply.started":"2026-01-31T21:35:15.622023Z","shell.execute_reply":"2026-01-31T21:35:15.636417Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Try downloading a different, popular traffic video\n!yt-dlp \"https://www.youtube.com/watch?v=MNn9qKG2UFI\" -o \"test_video.mp4\" --force-overwrites\n\nimport os\nif os.path.exists('test_video.mp4'):\n    print(\"‚úÖ Success! Video saved as 'test_video.mp4'\")\nelse:\n    print(\"‚ùå YouTube download failed. Please use Solution 1 (Add Input).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:35:15.638152Z","iopub.execute_input":"2026-01-31T21:35:15.638473Z","iopub.status.idle":"2026-01-31T21:35:15.825612Z","shell.execute_reply.started":"2026-01-31T21:35:15.638442Z","shell.execute_reply":"2026-01-31T21:35:15.824673Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nimport os\nimport glob\n\n# --- 1. Smart Image Search ---\n# We look for images RECURSIVELY (**) inside the input folder\ndataset_root = '/kaggle/input/rider-with-helmet-without-helmet-number-plate'\nextensions = ['*.jpg', '*.jpeg', '*.png']\nimage_files = []\n\nprint(\"üîç Searching for images...\")\nfor ext in extensions:\n    # Look in all subfolders\n    found = glob.glob(os.path.join(dataset_root, '**', ext), recursive=True)\n    image_files.extend(found)\n\n# Sort them to keep order and take the first 30\nimage_files = sorted(image_files)[:30]\n\nif len(image_files) == 0:\n    print(\"‚ùå CRITICAL ERROR: No images found! Please check if your dataset is attached.\")\n    print(f\"   Scanned path: {dataset_root}\")\nelse:\n    print(f\"‚úÖ Found {len(image_files)} images. Creating 'Simulation.mp4'...\")\n\n    # --- 2. Create the Video ---\n    video_name = 'Simulation.mp4'\n    frame_size = (640, 640) \n    fps = 1 # 1 frame per second\n    \n    # Initialize Video Writer\n    out = cv2.VideoWriter(video_name, cv2.VideoWriter_fourcc(*'mp4v'), fps, frame_size)\n\n    count = 0\n    for img_path in image_files:\n        img = cv2.imread(img_path)\n        \n        if img is not None:\n            # Resize to fit the video frame\n            img_resized = cv2.resize(img, frame_size)\n            out.write(img_resized)\n            count += 1\n        else:\n            print(f\"‚ö†Ô∏è Could not read image: {img_path}\")\n\n    out.release()\n    \n    if count > 0:\n        print(f\"üéâ Success! saved {count} frames to '{video_name}'\")\n        print(\"üëâ Now you can proceed to Code 2 (Detection).\")\n    else:\n        print(\"‚ùå Error: Video writer failed to save any frames.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:35:15.827220Z","iopub.execute_input":"2026-01-31T21:35:15.827602Z","iopub.status.idle":"2026-01-31T21:35:16.658769Z","shell.execute_reply.started":"2026-01-31T21:35:15.827567Z","shell.execute_reply":"2026-01-31T21:35:16.658104Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nfrom ultralytics import YOLO\nimport easyocr\nimport os\n\n# --- PATHS ---\nvideo_path = 'Simulation.mp4'  # The video we just made\nmodel_path = '/kaggle/working/runs/detect/helmet_project/real_run_fixed/weights/best.pt'\noutput_path = 'Final_Project_Demo.mp4'\n\n# IDs (Based on your training)\nVIOLATION_ID = 2\nPLATE_ID = 3\n\nprint(\"üöÄ Starting AI Detection on Simulation Video...\")\n\n# Initialize\nmodel = YOLO(model_path)\nreader = easyocr.Reader(['en'])\ncap = cv2.VideoCapture(video_path)\n\n# Video Properties\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\nfps = int(cap.get(cv2.CAP_PROP_FPS))\n\nout = cv2.VideoWriter(output_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n\nframe_count = 0\n\nwhile cap.isOpened():\n    success, frame = cap.read()\n    if not success:\n        break\n        \n    # 1. Run YOLO\n    results = model.predict(frame, conf=0.4, verbose=False)\n    result = results[0]\n    annotated_frame = result.plot() # Draw boxes\n    \n    # 2. Check for Violations & Read Plate\n    for box in result.boxes:\n        cls_id = int(box.cls[0])\n        \n        if cls_id == VIOLATION_ID:\n            # Violation found! Look for a plate in this frame\n            plate_text = \"\"\n            \n            for p_box in result.boxes:\n                if int(p_box.cls[0]) == PLATE_ID:\n                    x1, y1, x2, y2 = map(int, p_box.xyxy[0])\n                    \n                    # Crop & Preprocess\n                    plate_crop = frame[y1:y2, x1:x2]\n                    try:\n                        gray = cv2.cvtColor(plate_crop, cv2.COLOR_BGR2GRAY)\n                        # Upscale for better reading\n                        gray = cv2.resize(gray, None, fx=3, fy=3, interpolation=cv2.INTER_CUBIC)\n                        \n                        ocr_out = reader.readtext(gray, allowlist='ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n                        if ocr_out:\n                            plate_text = ocr_out[0][1]\n                            # Draw Plate Text on Screen\n                            cv2.putText(annotated_frame, f\"PLATE: {plate_text}\", (x1, y1-10), \n                                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n                    except:\n                        pass\n\n    out.write(annotated_frame)\n    frame_count += 1\n    print(f\"Processed Frame {frame_count}...\", end='\\r')\n\ncap.release()\nout.release()\nprint(f\"\\n\\n‚úÖ DONE! Download '{output_path}' from the Output folder.\")\n\n# 3. Convert for Browser Playback\nos.system(f\"ffmpeg -y -i {output_path} -vcodec libx264 Final_Browser_Preview.mp4\")\nfrom IPython.display import Video\ndisplay(Video(\"Final_Browser_Preview.mp4\", embed=True, width=800))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:35:16.659890Z","iopub.execute_input":"2026-01-31T21:35:16.660214Z","iopub.status.idle":"2026-01-31T21:35:20.831437Z","shell.execute_reply.started":"2026-01-31T21:35:16.660185Z","shell.execute_reply":"2026-01-31T21:35:20.830201Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import requests\nimport os\n\n# Pexels Video (Heavy Bike Traffic)\nurl = \"https://videos.pexels.com/video-files/854671/854671-hd_1280_720_25fps.mp4\"\noutput_file = \"real_traffic.mp4\"\n\nprint(f\"‚¨áÔ∏è Downloading with browser headers...\")\n\n# 1. Setup Headers to mimic Chrome\nheaders = {\n    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n    \"Referer\": \"https://www.pexels.com/\"\n}\n\n# 2. Stream the download\ntry:\n    with requests.get(url, stream=True, headers=headers) as r:\n        r.raise_for_status()\n        with open(output_file, 'wb') as f:\n            for chunk in r.iter_content(chunk_size=8192):\n                f.write(chunk)\n    \n    file_size = os.path.getsize(output_file) / (1024 * 1024)\n    print(f\"‚úÖ Success! Downloaded '{output_file}' ({file_size:.2f} MB)\")\n    print(\"üëâ Now re-run the 'Detection on Real Video' code.\")\n\nexcept Exception as e:\n    print(f\"‚ùå Download failed: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:35:20.832755Z","iopub.execute_input":"2026-01-31T21:35:20.833043Z","iopub.status.idle":"2026-01-31T21:35:21.211202Z","shell.execute_reply.started":"2026-01-31T21:35:20.833016Z","shell.execute_reply":"2026-01-31T21:35:21.210395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_video = '/kaggle/input/YOUR_NEW_DATASET/video.mp4'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:35:21.212192Z","iopub.execute_input":"2026-01-31T21:35:21.212516Z","iopub.status.idle":"2026-01-31T21:35:21.216248Z","shell.execute_reply.started":"2026-01-31T21:35:21.212488Z","shell.execute_reply":"2026-01-31T21:35:21.215650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nfrom ultralytics import YOLO\nimport easyocr\nimport os\n\n# --- 1. CONFIGURATION ---\n# Your uploaded video path\ninput_video = '/kaggle/input/video1/WhatsApp Video 2026-01-31 at 22.07.08.mp4'\n\n# Output filename (Simple name to avoid errors)\noutput_video = 'My_Result.mp4'\n\n# Your trained model\nmodel_path = '/kaggle/working/runs/detect/helmet_project/real_run_fixed/weights/best.pt'\n\n# Class IDs (Based on your training)\nVIOLATION_ID = 2  # No Helmet\nPLATE_ID = 3      # Number Plate\n\n# --- 2. VALIDATION ---\nif not os.path.exists(input_video):\n    print(f\"‚ùå Error: Could not find video at: {input_video}\")\n    print(\"üëâ Check if the folder name 'video1' is correct in the Right Sidebar.\")\nelse:\n    print(f\"üöÄ Found video! Processing: {input_video}\")\n\n    # --- 3. INITIALIZE ---\n    model = YOLO(model_path)\n    reader = easyocr.Reader(['en'])\n    cap = cv2.VideoCapture(input_video)\n\n    # Video Properties\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    \n    # Create Writer\n    out = cv2.VideoWriter(output_video, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n\n    frame_count = 0\n    \n    print(\"üé• Processing frames... (Please wait)\")\n\n    while cap.isOpened():\n        success, frame = cap.read()\n        if not success:\n            break\n            \n        # Run YOLO (Low confidence to catch everything)\n        results = model.predict(frame, conf=0.25, verbose=False)\n        result = results[0]\n        \n        # Draw all detections first (so you see what the model sees)\n        annotated_frame = result.plot()\n        \n        # Custom Logic: Find Violations & Read Plates\n        for box in result.boxes:\n            cls_id = int(box.cls[0])\n            \n            # If No Helmet (Class 2)\n            if cls_id == VIOLATION_ID:\n                # 1. Draw a thick red box\n                x1, y1, x2, y2 = map(int, box.xyxy[0])\n                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 0, 255), 4)\n                \n                # 2. Look for any plate in this frame\n                for p_box in result.boxes:\n                    if int(p_box.cls[0]) == PLATE_ID:\n                        px1, py1, px2, py2 = map(int, p_box.xyxy[0])\n                        \n                        # Crop & Read Plate\n                        plate_crop = frame[py1:py2, px1:px2]\n                        try:\n                            # Preprocess: Grayscale + Upscale\n                            gray = cv2.cvtColor(plate_crop, cv2.COLOR_BGR2GRAY)\n                            gray = cv2.resize(gray, None, fx=3, fy=3, interpolation=cv2.INTER_CUBIC)\n                            \n                            # Read Text\n                            ocr_out = reader.readtext(gray, allowlist='ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n                            if ocr_out:\n                                text = ocr_out[0][1]\n                                # Draw Green Text\n                                cv2.putText(annotated_frame, f\"PLATE: {text}\", (px1, py1-10), \n                                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n                        except:\n                            pass\n\n        out.write(annotated_frame)\n        frame_count += 1\n        if frame_count % 10 == 0:\n            print(f\"Processed {frame_count} frames...\", end='\\r')\n\n    cap.release()\n    out.release()\n    print(f\"\\n‚úÖ DONE! Video saved as '{output_video}'\")\n\n    # --- 4. PLAY IN BROWSER ---\n    print(\"üîÑ Converting for browser playback...\")\n    # We use the simple output name to avoid space issues in FFmpeg\n    os.system(f\"ffmpeg -y -i {output_video} -vcodec libx264 Browser_Video.mp4\")\n    \n    from IPython.display import Video\n    display(Video(\"Browser_Video.mp4\", embed=True, width=800))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:35:21.217276Z","iopub.execute_input":"2026-01-31T21:35:21.217570Z","iopub.status.idle":"2026-01-31T21:39:54.874955Z","shell.execute_reply.started":"2026-01-31T21:35:21.217546Z","shell.execute_reply":"2026-01-31T21:39:54.873586Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nfrom ultralytics import YOLO\nimport easyocr\nimport os\nimport glob\n\n# --- 1. AUTO-LOCATE THE VIDEO ---\n# Search for any .webm file in the helmet folder\nsearch_path = '/kaggle/input/helmet'\nvideo_files = glob.glob(os.path.join(search_path, '**', '*.webm'), recursive=True)\n\nif not video_files:\n    # Try searching for mp4 just in case\n    video_files = glob.glob(os.path.join(search_path, '**', '*.mp4'), recursive=True)\n\nif not video_files:\n    print(f\"‚ùå Error: No video files (.webm or .mp4) found in {search_path}\")\n    print(\"üëâ Please check the dataset name in the Right Sidebar.\")\nelse:\n    input_video = video_files[0] # Take the first video found\n    print(f\"üöÄ Found video! Processing: {input_video}\")\n\n    # --- 2. CONFIGURATION ---\n    output_video = 'Helmet_Detection_Result.mp4'\n    model_path = '/kaggle/working/runs/detect/helmet_project/real_run_fixed/weights/best.pt'\n\n    # IDs (Based on your training)\n    VIOLATION_ID = 2  # No Helmet\n    PLATE_ID = 3      # Number Plate\n\n    # --- 3. INITIALIZE ---\n    model = YOLO(model_path)\n    reader = easyocr.Reader(['en'])\n    cap = cv2.VideoCapture(input_video)\n\n    # Video Properties\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    \n    # Safety check for FPS (WebM sometimes returns 0 or huge numbers)\n    if fps == 0 or fps > 60:\n        fps = 25 \n\n    out = cv2.VideoWriter(output_video, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n\n    frame_count = 0\n    print(\"üé• Processing frames... (This might take a minute)\")\n\n    while cap.isOpened():\n        success, frame = cap.read()\n        if not success:\n            break\n            \n        # Run YOLO\n        # conf=0.25 is balanced. Lower it to 0.15 if boxes are missing.\n        results = model.predict(frame, conf=0.25, verbose=False)\n        result = results[0]\n        \n        annotated_frame = result.plot()\n        \n        # Check for Violations\n        for box in result.boxes:\n            cls_id = int(box.cls[0])\n            \n            # If No Helmet (Class 2)\n            if cls_id == VIOLATION_ID:\n                # 1. Draw Red Box\n                x1, y1, x2, y2 = map(int, box.xyxy[0])\n                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 0, 255), 4)\n                \n                # 2. Find Plate\n                for p_box in result.boxes:\n                    if int(p_box.cls[0]) == PLATE_ID:\n                        px1, py1, px2, py2 = map(int, p_box.xyxy[0])\n                        \n                        # Crop & Read\n                        plate_crop = frame[py1:py2, px1:px2]\n                        try:\n                            gray = cv2.cvtColor(plate_crop, cv2.COLOR_BGR2GRAY)\n                            gray = cv2.resize(gray, None, fx=3, fy=3, interpolation=cv2.INTER_CUBIC)\n                            \n                            ocr_out = reader.readtext(gray, allowlist='ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n                            if ocr_out:\n                                text = ocr_out[0][1]\n                                cv2.putText(annotated_frame, f\"PLATE: {text}\", (px1, py1-10), \n                                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n                        except:\n                            pass\n\n        out.write(annotated_frame)\n        frame_count += 1\n        if frame_count % 10 == 0:\n            print(f\"Processed {frame_count} frames...\", end='\\r')\n\n    cap.release()\n    out.release()\n    print(f\"\\n‚úÖ DONE! Video saved as '{output_video}'\")\n\n    # --- 4. PLAY IN BROWSER ---\n    print(\"üîÑ Converting for browser playback...\")\n    os.system(f\"ffmpeg -y -i {output_video} -vcodec libx264 Browser_Result.mp4\")\n    \n    from IPython.display import Video\n    display(Video(\"Browser_Result.mp4\", embed=True, width=800))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:39:54.876840Z","iopub.execute_input":"2026-01-31T21:39:54.877145Z","iopub.status.idle":"2026-01-31T21:40:04.634655Z","shell.execute_reply.started":"2026-01-31T21:39:54.877119Z","shell.execute_reply":"2026-01-31T21:40:04.633850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nfrom ultralytics import YOLO\nimport easyocr\nimport os\nimport glob\n\n# --- 1. AUTO-LOCATE THE VIDEO ---\n# Search for video files in the new folder\nsearch_path = '/kaggle/input/hhell1'\nvideo_files = glob.glob(os.path.join(search_path, '**', '*.webm'), recursive=True)\n\n# If no webm, look for mp4\nif not video_files:\n    video_files = glob.glob(os.path.join(search_path, '**', '*.mp4'), recursive=True)\n\nif not video_files:\n    print(f\"‚ùå Error: No video files found in {search_path}\")\n    print(\"üëâ Please check if the dataset is attached correctly in the Right Sidebar.\")\nelse:\n    input_video = video_files[0] # Take the first video found\n    print(f\"üöÄ Found video! Processing: {input_video}\")\n\n    # --- 2. CONFIGURATION ---\n    output_video = 'Helmet_Result_New.mp4'\n    model_path = '/kaggle/working/runs/detect/helmet_project/real_run_fixed/weights/best.pt'\n\n    # IDs (Based on your training)\n    VIOLATION_ID = 2  # No Helmet\n    PLATE_ID = 3      # Number Plate\n\n    # --- 3. INITIALIZE ---\n    model = YOLO(model_path)\n    reader = easyocr.Reader(['en'])\n    cap = cv2.VideoCapture(input_video)\n\n    # Video Properties\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    \n    # Safety check for invalid FPS (common in webm files)\n    if fps <= 0 or fps > 60:\n        fps = 25 \n\n    out = cv2.VideoWriter(output_video, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n\n    frame_count = 0\n    print(\"üé• Processing frames... (This might take a minute)\")\n\n    while cap.isOpened():\n        success, frame = cap.read()\n        if not success:\n            break\n            \n        # Run YOLO \n        # Using conf=0.25 to catch smaller/faster objects\n        results = model.predict(frame, conf=0.25, verbose=False)\n        result = results[0]\n        \n        annotated_frame = result.plot()\n        \n        # Check for Violations\n        for box in result.boxes:\n            cls_id = int(box.cls[0])\n            \n            # If No Helmet (Class 2)\n            if cls_id == VIOLATION_ID:\n                # 1. Draw Red Box\n                x1, y1, x2, y2 = map(int, box.xyxy[0])\n                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 0, 255), 4)\n                \n                # 2. Find Plate\n                for p_box in result.boxes:\n                    if int(p_box.cls[0]) == PLATE_ID:\n                        px1, py1, px2, py2 = map(int, p_box.xyxy[0])\n                        \n                        # Crop & Read\n                        plate_crop = frame[py1:py2, px1:px2]\n                        try:\n                            gray = cv2.cvtColor(plate_crop, cv2.COLOR_BGR2GRAY)\n                            gray = cv2.resize(gray, None, fx=3, fy=3, interpolation=cv2.INTER_CUBIC)\n                            \n                            ocr_out = reader.readtext(gray, allowlist='ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n                            if ocr_out:\n                                text = ocr_out[0][1]\n                                cv2.putText(annotated_frame, f\"PLATE: {text}\", (px1, py1-10), \n                                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n                        except:\n                            pass\n\n        out.write(annotated_frame)\n        frame_count += 1\n        if frame_count % 10 == 0:\n            print(f\"Processed {frame_count} frames...\", end='\\r')\n\n    cap.release()\n    out.release()\n    print(f\"\\n‚úÖ DONE! Video saved as '{output_video}'\")\n\n    # --- 4. PLAY IN BROWSER ---\n    print(\"üîÑ Converting for browser playback...\")\n    os.system(f\"ffmpeg -y -i {output_video} -vcodec libx264 Browser_Result_New.mp4\")\n    \n    from IPython.display import Video\n    display(Video(\"Browser_Result_New.mp4\", embed=True, width=800))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:40:04.635891Z","iopub.execute_input":"2026-01-31T21:40:04.636188Z","iopub.status.idle":"2026-01-31T21:40:32.315573Z","shell.execute_reply.started":"2026-01-31T21:40:04.636162Z","shell.execute_reply":"2026-01-31T21:40:32.314409Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import cv2\nfrom ultralytics import YOLO\nimport easyocr\nimport os\nimport glob\n\n# --- 1. AUTO-LOCATE THE VIDEO ---\n# Search for video files in the new folder\nsearch_path = '/kaggle/input/one-plus-1'\n# Look for common video formats\nvideo_files = glob.glob(os.path.join(search_path, '**', '*.mp4'), recursive=True)\nif not video_files:\n    video_files = glob.glob(os.path.join(search_path, '**', '*.webm'), recursive=True)\nif not video_files:\n    video_files = glob.glob(os.path.join(search_path, '**', '*.mov'), recursive=True)\n\nif not video_files:\n    print(f\"‚ùå Error: No video files found in {search_path}\")\n    print(\"üëâ Please check if the dataset is attached correctly in the Right Sidebar.\")\nelse:\n    input_video = video_files[0] # Take the first video found\n    print(f\"üöÄ Found video! Processing: {input_video}\")\n\n    # --- 2. CONFIGURATION ---\n    output_video = 'Helmet_Result_OnePlus.mp4'\n    model_path = '/kaggle/working/runs/detect/helmet_project/real_run_fixed/weights/best.pt'\n\n    # IDs (Based on your training)\n    VIOLATION_ID = 2  # No Helmet\n    PLATE_ID = 3      # Number Plate\n\n    # --- 3. INITIALIZE ---\n    model = YOLO(model_path)\n    reader = easyocr.Reader(['en'])\n    cap = cv2.VideoCapture(input_video)\n\n    # Video Properties\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    \n    # Safety check for invalid FPS\n    if fps <= 0 or fps > 60:\n        fps = 25 \n\n    out = cv2.VideoWriter(output_video, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n\n    frame_count = 0\n    print(\"üé• Processing frames... (This might take a minute)\")\n\n    while cap.isOpened():\n        success, frame = cap.read()\n        if not success:\n            break\n            \n        # Run YOLO \n        results = model.predict(frame, conf=0.25, verbose=False)\n        result = results[0]\n        \n        annotated_frame = result.plot()\n        \n        # Check for Violations\n        for box in result.boxes:\n            cls_id = int(box.cls[0])\n            \n            # If No Helmet (Class 2)\n            if cls_id == VIOLATION_ID:\n                # 1. Draw Red Box\n                x1, y1, x2, y2 = map(int, box.xyxy[0])\n                cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), (0, 0, 255), 4)\n                \n                # 2. Find Plate\n                for p_box in result.boxes:\n                    if int(p_box.cls[0]) == PLATE_ID:\n                        px1, py1, px2, py2 = map(int, p_box.xyxy[0])\n                        \n                        # Crop & Read\n                        plate_crop = frame[py1:py2, px1:px2]\n                        try:\n                            gray = cv2.cvtColor(plate_crop, cv2.COLOR_BGR2GRAY)\n                            gray = cv2.resize(gray, None, fx=3, fy=3, interpolation=cv2.INTER_CUBIC)\n                            \n                            ocr_out = reader.readtext(gray, allowlist='ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n                            if ocr_out:\n                                text = ocr_out[0][1]\n                                cv2.putText(annotated_frame, f\"PLATE: {text}\", (px1, py1-10), \n                                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n                        except:\n                            pass\n\n        out.write(annotated_frame)\n        frame_count += 1\n        if frame_count % 10 == 0:\n            print(f\"Processed {frame_count} frames...\", end='\\r')\n\n    cap.release()\n    out.release()\n    print(f\"\\n‚úÖ DONE! Video saved as '{output_video}'\")\n\n    # --- 4. PLAY IN BROWSER ---\n    print(\"üîÑ Converting for browser playback...\")\n    os.system(f\"ffmpeg -y -i {output_video} -vcodec libx264 Browser_Result_OnePlus.mp4\")\n    \n    from IPython.display import Video\n    display(Video(\"Browser_Result_OnePlus.mp4\", embed=True, width=800))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:40:32.317177Z","iopub.execute_input":"2026-01-31T21:40:32.317624Z","iopub.status.idle":"2026-01-31T21:40:51.541567Z","shell.execute_reply.started":"2026-01-31T21:40:32.317576Z","shell.execute_reply":"2026-01-31T21:40:51.540388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install streamlit -q\n!npm install localtunnel -g","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:40:51.542997Z","iopub.execute_input":"2026-01-31T21:40:51.543715Z","iopub.status.idle":"2026-01-31T21:40:56.827545Z","shell.execute_reply.started":"2026-01-31T21:40:51.543667Z","shell.execute_reply":"2026-01-31T21:40:56.826775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile app.py\nimport streamlit as st\nimport cv2\nimport tempfile\nimport os\nfrom ultralytics import YOLO\nimport easyocr\nimport numpy as np\n\n# --- 1. SETUP & CONFIG ---\nst.set_page_config(page_title=\"Helmet Guard AI\", page_icon=\"üèçÔ∏è\")\n\nst.title(\"üèçÔ∏è AI Traffic Cop: Helmet Detection System\")\nst.markdown(\"\"\"\n**Upload a traffic video**, and this AI will:\n1. Detect riders without helmets (üî¥ Red Box).\n2. Capture their license plates (üü¢ Green Text).\n\"\"\")\n\n# Path to your best trained model (adjust if needed)\nMODEL_PATH = '/kaggle/working/runs/detect/helmet_project/real_run_fixed/weights/best.pt'\n\n# --- 2. LOAD MODEL ---\n@st.cache_resource\ndef load_model():\n    return YOLO(MODEL_PATH)\n\ntry:\n    model = load_model()\n    reader = easyocr.Reader(['en'], gpu=True)\n    st.success(\"‚úÖ Model Loaded Successfully!\")\nexcept Exception as e:\n    st.error(f\"‚ùå Error loading model: {e}\")\n    st.stop()\n\n# --- 3. HELPER FUNCTION TO PROCESS VIDEO ---\ndef process_video(video_path):\n    cap = cv2.VideoCapture(video_path)\n    \n    # Get video properties\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n    \n    # Define Output Path\n    output_path = \"output_detected.mp4\"\n    \n    # We use 'avc1' (H.264) which works better in web browsers than 'mp4v'\n    fourcc = cv2.VideoWriter_fourcc(*'avc1') \n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n    # Progress Bar\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n    progress_bar = st.progress(0)\n    status_text = st.empty()\n    \n    frame_count = 0\n    \n    while cap.isOpened():\n        success, frame = cap.read()\n        if not success:\n            break\n            \n        # Run Detection\n        results = model.predict(frame, conf=0.25, verbose=False)\n        result = results[0]\n        annotated_frame = result.plot()\n        \n        # Check for Violations (Class 2 = No Helmet)\n        violation_found = False\n        for box in result.boxes:\n            if int(box.cls[0]) == 2:\n                violation_found = True\n                \n        # If violation, look for plate (Class 3)\n        if violation_found:\n            for box in result.boxes:\n                if int(box.cls[0]) == 3:\n                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n                    \n                    # Crop & Read\n                    plate_crop = frame[y1:y2, x1:x2]\n                    try:\n                        gray = cv2.cvtColor(plate_crop, cv2.COLOR_BGR2GRAY)\n                        gray = cv2.resize(gray, None, fx=3, fy=3, interpolation=cv2.INTER_CUBIC)\n                        \n                        ocr_out = reader.readtext(gray, allowlist='ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789')\n                        if ocr_out:\n                            text = ocr_out[0][1]\n                            cv2.putText(annotated_frame, f\"PLATE: {text}\", (x1, y1-10), \n                                        cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)\n                    except:\n                        pass\n\n        out.write(annotated_frame)\n        frame_count += 1\n        \n        # Update UI every 10 frames\n        if frame_count % 10 == 0:\n            progress = min(frame_count / total_frames, 1.0)\n            progress_bar.progress(progress)\n            status_text.text(f\"Processing Frame {frame_count}/{total_frames}...\")\n\n    cap.release()\n    out.release()\n    return output_path\n\n# --- 4. MAIN UI ---\nuploaded_file = st.file_uploader(\"Upload a Video (MP4/WebM)\", type=['mp4', 'mov', 'avi', 'webm'])\n\nif uploaded_file is not None:\n    # Save uploaded file to temp\n    tfile = tempfile.NamedTemporaryFile(delete=False) \n    tfile.write(uploaded_file.read())\n    video_path = tfile.name\n    \n    st.video(video_path) # Show original\n    \n    if st.button(\"üöÄ Start Detection\"):\n        with st.spinner(\"Analyzing traffic... Please wait.\"):\n            try:\n                out_video = process_video(video_path)\n                st.success(\"Processing Complete!\")\n                \n                # We need to re-encode with ffmpeg to ensure browser compatibility\n                # because OpenCV's writer is sometimes tricky with web players\n                os.system(f\"ffmpeg -y -i {out_video} -vcodec libx264 final_browser_ready.mp4\")\n                \n                st.header(\"üéØ Detection Results\")\n                st.video(\"final_browser_ready.mp4\")\n                \n            except Exception as e:\n                st.error(f\"An error occurred: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:40:56.829266Z","iopub.execute_input":"2026-01-31T21:40:56.829534Z","iopub.status.idle":"2026-01-31T21:40:56.838102Z","shell.execute_reply.started":"2026-01-31T21:40:56.829504Z","shell.execute_reply":"2026-01-31T21:40:56.837557Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import subprocess\nimport time\nimport sys\nimport os\nimport re\n\n# --- 1. SETUP & FIX DEPENDENCIES ---\nprint(\"üîß Fixing dependencies for Cloud Environment...\")\n# We FORCE install the 'headless' version of OpenCV which doesn't need a screen\nsubprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"opencv-python-headless\", \"pyngrok\", \"streamlit\"], stdout=subprocess.DEVNULL)\n\n# --- 2. CLEANUP ---\nprint(\"üßπ Cleaning up old processes...\")\nos.system(\"pkill -f streamlit\")\nos.system(\"pkill -f cloudflared\")\n\n# --- 3. START STREAMLIT (With Error Logging) ---\nprint(\"‚è≥ Starting Streamlit App (Logging to 'app_log.txt')...\")\n\n# Command to run the app\ncmd = [sys.executable, \"-m\", \"streamlit\", \"run\", \"app.py\", \"--server.headless=true\", \"--server.address=localhost\"]\n\n# Open a log file to capture errors\nwith open(\"app_log.txt\", \"w\") as log_file:\n    # Run in background\n    streamlit_process = subprocess.Popen(cmd, stdout=log_file, stderr=log_file)\n\n# --- 4. WAIT & VERIFY (The Critical Step) ---\nprint(\"üîç Waiting for App to start...\", end=\"\")\napp_ready = False\n\nfor i in range(30):  # Wait up to 30 seconds\n    if streamlit_process.poll() is not None:\n        print(\"\\n‚ùå CRITICAL: Streamlit crashed immediately!\")\n        break\n        \n    try:\n        # Check if server is responding on port 8501\n        subprocess.check_output([\"curl\", \"-s\", \"http://localhost:8501\"])\n        app_ready = True\n        print(\" ‚úÖ App is Alive!\")\n        break\n    except:\n        time.sleep(1)\n        if i % 5 == 0: print(\".\", end=\"\")\n\n# --- 5. RESULT ---\nif not app_ready:\n    print(\"\\n\\n‚ùå ERROR: App failed to start. Here is the REAL Error Log:\")\n    print(\"-----------------------------------------------------\")\n    try:\n        with open(\"app_log.txt\", \"r\") as f:\n            print(f.read())\n    except:\n        print(\"Could not read log file.\")\n    print(\"-----------------------------------------------------\")\n    print(\"üëâ Please copy the error message above and paste it here.\")\nelse:\n    # --- 6. START CLOUDFLARE TUNNEL ---\n    # Only start the tunnel if the app is actually running!\n    print(\"üöÄ App is healthy! Starting Cloudflare Tunnel...\")\n    \n    if not os.path.exists(\"cloudflared\"):\n        subprocess.run(\"wget -q -O cloudflared https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\", shell=True)\n        subprocess.run(\"chmod +x cloudflared\", shell=True)\n\n    with open(\"cf_logs.txt\", \"w\") as log_file:\n        cf_process = subprocess.Popen(\n            [\"./cloudflared\", \"tunnel\", \"--url\", \"http://localhost:8501\"],\n            stdout=log_file,\n            stderr=log_file\n        )\n\n    print(\"üîó Generating Link...\", end=\"\")\n    public_url = None\n    for i in range(20):\n        time.sleep(1)\n        print(\".\", end=\"\")\n        try:\n            with open(\"cf_logs.txt\", \"r\") as f:\n                logs = f.read()\n                match = re.search(r\"https://[a-zA-Z0-9-]+\\.trycloudflare\\.com\", logs)\n                if match:\n                    public_url = match.group(0)\n                    break\n        except:\n            pass\n\n    if public_url:\n        print(\"\\n\\nüéâ --------------------------------------------------\")\n        print(f\"üöÄ YOUR APP IS LIVE HERE: {public_url}\")\n        print(\"--------------------------------------------------\\n\")\n        print(\"‚ö†Ô∏è Keep this cell running!\")\n        \n        try:\n            while True:\n                time.sleep(1)\n        except KeyboardInterrupt:\n            print(\"\\nüõë Shutting down...\")\n            streamlit_process.terminate()\n            cf_process.terminate()\n    else:\n        print(\"\\n‚ùå Error: Could not get Cloudflare link. Check 'cf_logs.txt'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-31T21:51:06.331815Z","iopub.execute_input":"2026-01-31T21:51:06.332090Z","iopub.status.idle":"2026-01-31T21:51:15.341304Z","shell.execute_reply.started":"2026-01-31T21:51:06.332059Z","shell.execute_reply":"2026-01-31T21:51:15.340692Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}